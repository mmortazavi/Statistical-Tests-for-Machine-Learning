{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Statistical Tests for Machine Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Basic Concepts\n",
    "\n",
    "   - Hypothesis testing: Statistical problem which, in regard to a data sample, assumes that an initial hypothesis  H0  is true and check if the evidence against it is sufficiently strong in favor of an alternative hypothesis  H1.\n",
    "\n",
    "   - Statistic: Quantitative measure calculated with the tests. This quantitative measure provides the feasibility of the null hypothesis. Each test has its characteristic way of obtaining this value and the statistics follow a particular probability distribution (such as normal distribution, etc.).    \n",
    "   - Confidence Internal (CI): A confidence interval is how much uncertainty there is with any particular statistic. Confidence intervals are often used with a margin of error. It tells you how confident you can be that the results from a poll or survey reflect what you would expect to find if it were possible to survey the entire population. Confidence intervals are intrinsically connected to confidence levels α or Type I error (false positive) (A 0% confidence level means you have no faith at all that if you repeated the survey that you would get the same results. A 100% confidence level means there is no doubt at all that if you repeated the survey you would get the same results). Factors that Affect CI are: Population size, Sample Size, Percentage. Below α is the error quantile.\n",
    "    ![image.png](http://www.statisticshowto.com/wp-content/uploads/2009/10/construct-a-confidence-interval.gif)\n",
    "    For a 95% confidence level using normal distribution (α=5%):\n",
    "    ![image.png](https://www.mathsisfun.com/data/images/ci95.gif)\n",
    "*Note finding confidence intervals is not only based on the normal distribution. In reality, most confidence intervals are found using the t-distribution (especially if you are working with small samples). <br>\n",
    "*For an example of how it is computed in machine learning for a classification problem see: https://sebastianraschka.com/blog/2016/model-evaluation-selection-part1.html or https://machinelearningmastery.com/report-classifier-performance-confidence-intervals/\n",
    "   - p-value: Probability of obtaining a value of the statistic at least as extreme as the obtained in the test. Learn more.\n",
    "   - Rejection of  H0: When the p-value is less than the significance level  α, the test is considered statistically significant and the null hypothesis is rejected."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Parametric vs Non-parametric\n",
    "\n",
    "### Parametric test: \n",
    "Statistical test that makes assumptions about the parameters of the population distribution of the data. These assumptions can be summarize as:\n",
    "- Normality: The distribution of the data follows a gaussian distribution.\n",
    "- Homoscedasticity: This assumption means that the variance around the regression line is the same for all values of the predictor variable.\n",
    "- Independence: the values of a group are independent between them.\n",
    "    \n",
    "### Non-parametric test: \n",
    "Statistical test that does not make assumptions about the parameters of the population distribution of the data. Less powerful than its parametric counterpart. Nonparametric tests are sometimes called distribution-free tests because they are based on fewer assumptions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Normality Tests:\n",
    "\n",
    "- Shapiro-Wilk: Tests the null hypothesis that the samples come from a normally distributed population. This is considered as one of the most powerful tests with unspecified mean and variance, especially for samples of less than 30 elements.\n",
    "\n",
    "- D'Agostino-Pearson: Tests the null hypothesis that the samples come from normally distributed population. The test combines the asymmetry coefficient (to what extent the normal is symmetric or of coefficient 0) and the coefficient of Kurtosis (degree of amplitude, usually of coefficient 0) in order to obtain an statistic and p-value. It first computes the skewness and kurtosis to quantify how far the distribution is from Gaussian in terms of asymmetry and shape. It then calculates how far each of these values differs from the value expected with a Gaussian distribution, and computes a single P value from the sum of these discrepancies. \n",
    "- Chi-square:  You can use a chi square test for normality. The advantage is that it's relatively easy to use, but it isn't a very strong test. If you have a small sample (under 20), it may be the only test you can use. For larger samples, you're much better off choosing another option.\n",
    "- Kolmogorov-Smirnov: Performs a test of goodness of fit, to determine if the data follows a normal distribution. It assumes, as H0, that the distribution obtained from the observed data is identical to the normal distribution. This is the least powerful offering the worst results off the three. It makes use of the properties of the cumulative probability distribution to compare any two distributions, in this case your samples, and a normal distribution.\n",
    "\n",
    "- Also there are very simple ways for normality test: \n",
    "    - Boxplots: Draw a boxplot of your data. If your data comes from a normal distribution, the box will be symmetrical with the mean and median in the center. If the data meets the assumption of normality, there should also be few outliers.\n",
    "    ![image.png](http://www.statisticshowto.com/wp-content/uploads/2016/03/normal-boxplot-2.png)\n",
    "    - Q-Q Plots (Quantile-Quantile plots): is a scatterplot created by plotting two sets of quantiles against one another. If both sets of quantiles came from the same distribution, we should see the points forming a line that’s roughly straight. Here’s an example of a Normal Q-Q plot when both sets of quantiles truly come from Normal distributions.\n",
    "      ![image.png](https://data.library.virginia.edu/files/randu.jpeg)\n",
    "\n",
    "    - Histogram plots:\n",
    "      ![image.png](https://www.itl.nist.gov/div898/handbook/eda/gif/histogr1.gif)\n",
    "\n",
    "   \n",
    "*A common misconception: Majority (if not all) tests that rely on normality. Your target does not need to follow normal distribution. Only the residuals (difference between observed and predicted values) need to follow a Gaussian distribution to validate the hypothesis tests. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Homoscedasticity Test:\n",
    "\n",
    "- Levene: Tests the null hypothesis that all the input populations come from populations with equal variances. If the p-value obtained from Levene's test is less than the significance level, the hypothesis is rejected. Parametric tests such as Anova or T-test, assume homoscedasticity in the populations.\n",
    "\n",
    "![image.png](https://i2.wp.com/statisticsbyjim.com/wp-content/uploads/2017/08/residuals_unfixed.png?w=576)\n",
    "\n",
    "Tool: https://docs.scipy.org/doc/scipy-0.14.0/reference/generated/scipy.stats.levene.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Independence/Colleration Test:\n",
    "\n",
    "It is a bivariate analysis that measures the strength of association between two variables and the direction of the relationship. \n",
    "\n",
    "For continuous variables:\n",
    "- Pearson r correlation: It is the most widely used correlation statistic to measure the degree of the relationship between linearly related variables.  For example, in the stock market, if we want to measure how two stocks are related to each other, Pearson r correlation is used to measure the degree of relationship between the two. Both variables should be normally distributed as well as assumptions like linearity and homoscedasticity.  It can asnwers questions like: \n",
    "    - Is there a statistically significant relationship between age, as measured in years, and height, measured in inches?\n",
    "    - Is there a relationship between temperature, measured in degrees Fahrenheit, and ice cream sales, measured by income?\n",
    "- Spearman: Tests for the strength of the association between two ordinal variables (does not rely on the assumption of normally distributed data). Pearson benchmarks linear relationship, Spearman benchmarks monotonic relationship. If you want to explore your data it is best to compute both, since the relation between the Spearman (S) and Pearson (P) correlations will give some information. Briefly, S is computed on ranks and so depicts monotonic relationships while P is on true values and depicts linear relationships.\n",
    "\n",
    "\n",
    "- Kendall: This correlation coefficient we will discuss is also based on variable ranks. However, unlike Spearman's coefficient, Kendall's τ does not take into account the difference between ranks — only directional agreement. Therefore, this coefficient is more appropriate for discrete data.\n",
    "\n",
    "For categorical variables:\n",
    "- Chi-Square Test: It tests the independence of two categorical variables. The test makes use of the assumption that the square of a Standard Normal Distribution follows a Chi Squared Distribution. The following hypothesis statements:\n",
    "    - Null Hypothesis: The two categorical variables are independent.\n",
    "    - Alternative Hypothesis: The two categorical variables are dependent.\n",
    "    \n",
    "*Please note that Chi-Square Test is also used for Goodness-of-Fit to test whether random categorical variables follow a particular probability distribution!\n",
    "\n",
    "\n",
    "** Correlation and Causation:\n",
    "The relationships between variables in our fuel efficiency example were very intuitive and explainable through vehicle mechanics. However, things are not always this straightforward. It is a well-known fact that correlation does not imply causation, and therefore, any strong correlation should be thought of critically. \n",
    "\n",
    "ref: https://www.datascience.com/learn-data-science/fundamentals/introduction-to-correlation-python-data-science"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## 6. Parametric two groups tests (e.g. Comparison of Means)\n",
    "\n",
    "- t-test: Tests the null hypothesis that 2 related or repeated samples have identical mean values with assumption of equal variance. The test checks if the mean score differs significantly between the samples. It can be applied to:\n",
    "    - Paired data: the data values of each group are related.\n",
    "    - Unpaired data: the data values of each group are independent.\n",
    "\n",
    "- Welch's t-test: Welch's t-test is an adaptation of Student's t-test, that is more reliable when the two samples have unequal variances and unequal sample sizes.  The modification is to the degrees of freedom used in the test, which tends to increase the test power for samples with unequal variance.\n",
    "\n",
    "- F-test: F-distribution is the distribution of ratios of two independent estimators of the population variances. The main use of F-distribution is to test whether two independent samples have been drawn for the normal populations with the same variance, or if two independent estimates of the population variance are homogeneous or not, since it is often desirable to compare two variances rather than two averages."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## 7. Parametric multiple groups tests\n",
    "- ANOVA: Tests the null hypothesis that the means of the results of two or more groups are the same. For this, the test analyzes the variation between samples as well as their inner variation with the variance. The statistic of the ANOVA test, is estimated by the f-distribution.\n",
    "    - One-Way : When we compare more than two groups, based on one factor (independent variable), this is called one way ANOVA. For example, it is used if a manufacturing company wants to compare the productivity of three or more employees based on working hours. This is called one way ANOVA.\n",
    "    - Two-Way : When a company wants to compare the employee productivity based on two factors (2 independent variables), then it said to be two way (Factorial) ANOVA. For example, based on the working hours and working conditions, if a company wants to compare employee productivity, it can do that through two way ANOVA. \n",
    "    \n",
    "    *Effect Size: When conducting an ANOVA it is always important to calculate the effect size. The effect size can tell you the degree to which the null hypothesis is false. Effect sizes can be considered small, medium or large. Some methods are: Eta  squared (h2), partial Eta squared (hp2), omega squared (w2), and the Intraclass correlation (rI).\n",
    "    \n",
    "- Bonferroni: Once evidence of the existence of significant differences between the means of the algorithms is achieved, thanks to the variance analysis of ANOVA, it is possible to proceed with the POST-HOC test of Bonferroni in order to determine the discrepancies between all the samples, comparing the means of all the algorithms. Each p-value associated with the hypothesis  Hi  is compared taking an  α  adjusted to all the comparisons:  pi<αm , where  K  is the number of algorithms and m is the number of comparisons:  m=K∗(K−1)2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Non-parametric two groups tests\n",
    "\n",
    "- Wilcoxon: Tests the null hypothesis that two related paired samples come from the same distribution. It assumes that the differences between samples are symmetrical with respect to the median. \n",
    "- Binomial Sign: Tests the null hypothesis that two related paired samples come from the same distribution. It does not assume symmetry but is less powerfull than Wilcoxon. \n",
    "- Mann-Whitney-U: Tests the null hypothesis that two non-paired samples come from the same distribution.\n",
    "- Anderson–Darling test: The test is a modification of the Kolmogorov- Smirnov test kstest for the null hypothesis that a sample is drawn from a population that follows a particular distribution. For the Anderson-Darling test, the critical values depend on which distribution is being tested against. This function works for normal, exponential, logistic, or Gumbel (Extreme Value Type I) distributions.\n",
    "- Sign test: The test is the simplest nonparametric test for matched or paired data. The approach is to analyze only the signs of the difference scoresTests if two related variables are different—ignores the magnitude of change, only takes into account direction\n",
    "- Wilcoxon Signed Rank Test: Tests for the difference between two related variables—takes into account the magnitude and direction of difference. It can be used as an alternative to the paired Student's t-test, t-test for matched pairs, or the t-test for dependent samples when the population cannot be assumed to be normally distributed.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## 9. Non-parametric multiple groups tests\n",
    "There is also other non-parametric multiple groups tests, but much less frequently used like:\n",
    "- Friedman: This test makes comparisons and assigns rankings to each data set. The statistic follows a chis-quared distribution with  K−1  degrees of freedom, being  K  the number of related variables (or number of algorithms).\n",
    "\n",
    "- Friedman's aligned ranks: It makes comparisons an assigns rankings considering all the data sets. It is usually employed when the number of algorithms in the comparison is low.\n",
    "\n",
    "- Quade: Similar to ImanDavenport, only that it takes into account that some problems are more difficult or that the results obtained from different algorithms present higher discrepancies (weighting)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Minimum sample size for Statistical Analysis:\n",
    "Ref: http://www.dummies.com/education/math/statistics/how-to-determine-the-minimum-size-needed-for-a-statistical-sample/\n",
    "\n",
    "The margin of error of a confidence interval (CI) is affected by size of the statistical sample; as the size increases, margin of error decreases. Looking at this the other way around, if you want a smaller margin of error (and doesn’t everyone?), you need a larger sample size.\n",
    "\n",
    "Suppose you are getting ready to do your own survey to estimate a population mean; wouldn’t it be nice to see ahead of time what sample size you need to get the margin of error you want? Thinking ahead will save you money and time and it will give you results you can live with in terms of the margin of error — you won’t have any surprises later.\n",
    "\n",
    "The sample size required to get a desired margin of error (MOE) when you are doing a confidence interval:\n",
    "\n",
    "![Image of Yaktocat](http://d2r5da613aq50s.cloudfront.net/wp-content/uploads/359816.image0.png)\n",
    "\n",
    "In this formula, MOE is the number representing the margin of error you want, and z* is the z*-value corresponding to your desired confidence level (faccording to normal distribution, most people use 1.96 for a 95% confidence interval)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11. Choose Right Test based on the research design, variable type, and distribution?\n",
    "\n",
    "The chart below provides a summary of the questions you need to answer before you can choose the right test.\n",
    "\n",
    "![Image of Yaktocat](https://cyfar.org/sites/default/files/choosing%20the%20right%20test%203.14.11_0.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example Application: Kolmogorov–Smirnov test (KS Test ):\n",
    "\n",
    "An issue with Student’s T-Test, samples must be normal (shaped in a normal distribution). That is an issue for us because often at work there are distributions other than normal like Poisson distributions. KS test allows you to detect patterns you can’t detect with a Student’s T-Test. An example that shows the difference between Student’s T-Test and KS Test:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Image of Yaktocat](https://cdn-images-1.medium.com/max/1600/0*_zFg_-LPurj7FbPL.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Because the sample mean and standard deviation are highly similar the Student’s T-Test gives a very high p-value. KS Test can detect the variance. In this case the red distribution has a slightly binomial distribution which KS detect. In other words:\n",
    "- Student’s T-Test says that there is 79.3% chances the two samples come from the same distribution.\n",
    "- KS Test says that there are 1.6% chances the two samples come from the same distribution.\n",
    "\n",
    "## Mix of Graph Network and KS Test for Clustering (advanced)!\n",
    "ref: https://towardsdatascience.com/kolmogorov-smirnov-test-84c92fb4158d"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
